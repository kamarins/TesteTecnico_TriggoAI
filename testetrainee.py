# -*- coding: utf-8 -*-
"""TesteTrainee.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_gHHx9FkbC5ooC-OeDwMYsMiagPIBSxp

#**Teste Técnico - Programa Trainee triggo.ai**

Por Karine Marins

##1. Bibliotecas
"""

# Importando bibliotecas necessárias
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import os # accessing directory structure
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans

"""##2. Importando base de dados"""

#Importando base de dados
import kagglehub

# Download latest version
path = kagglehub.dataset_download("olistbr/brazilian-ecommerce")

print("Path to dataset files:", path)

#Acessando base de dados
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

df_customers=pd.read_csv('/kaggle/input/brazilian-ecommerce/olist_customers_dataset.csv')

df_sellers=pd.read_csv('/kaggle/input/brazilian-ecommerce/olist_sellers_dataset.csv')

df_order_reviews=pd.read_csv('/kaggle/input/brazilian-ecommerce/olist_order_reviews_dataset.csv')

df_order_items=pd.read_csv('/kaggle/input/brazilian-ecommerce/olist_order_items_dataset.csv')

df_products=pd.read_csv('/kaggle/input/brazilian-ecommerce/olist_products_dataset.csv')

df_geolocation=pd.read_csv('/kaggle/input/brazilian-ecommerce/olist_geolocation_dataset.csv')

df_products_category=pd.read_csv('/kaggle/input/brazilian-ecommerce/product_category_name_translation.csv')

df_orders=pd.read_csv('/kaggle/input/brazilian-ecommerce/olist_orders_dataset.csv')

df_order_payments=pd.read_csv('/kaggle/input/brazilian-ecommerce/olist_order_payments_dataset.csv')

"""##3. Análises para cada df

### 3.1 df_costumers
"""

#verificando formato do dataframe
df_customers.shape

#verificando as informações do dataframe
df_customers.info()

#verificando os dados
df_customers.head(10)

#Procurando por dados faltantes
#retorna true caso haja  valores do tipo NaN ou Nulo
df_customers.isnull()

#como não obtivemos resposta concreta, vamos utilizar outro metodo que soma a quantidade de registros faltantes
df_customers.isnull().sum()

"""NÃO HÁ DADOS FALTANTES!"""

# Removendo duplicatas
df_customers = df_customers.drop_duplicates()

"""### 3.2 df_geolocation"""

#verificando formato do dataframe
df_geolocation.shape

#verificando informações do dataframe
df_geolocation.info()

#observando os dados
df_geolocation.head(10)

#Procurando por dados faltantes
#retorna true caso haja  valores do tipo NaN ou Nulo
df_geolocation.isnull()

df_geolocation.isnull().sum()

df_geolocation.isna().sum()

"""NÃO HÁ DADOS FALTANTES!"""

# Removendo duplicatas
df_geolocation = df_geolocation.drop_duplicates()

"""###3.3 df_order_items

"""

#verificando formato do dataframe
df_order_items.shape

#verificando informações presentes no dataframe
df_order_items.info()

#verificando os dados
df_order_items.head(10)

#Procurando por dados faltantes
#retorna true caso haja  valores do tipo NaN ou Nulo
df_order_items.isnull()

df_order_items.isnull().sum()

"""NÃO HÁ DADOS FALTANTES!"""

# Removendo duplicatas
df_order_items = df_order_items.drop_duplicates()

"""###3.4 df_order_payments"""

#verificando as informações presentes no df
 df_order_payments.info()

#observando como são os dados
df_order_payments.head(10)

#verificando existencia de dados nulos
df_order_payments.isnull()

df_order_payments.isnull().sum()

"""NÃO POSSUI DADOS NULOS!"""

# Removendo duplicatas
df_order_payments = df_order_payments.drop_duplicates()

"""###3.5 df_order_reviews"""

#verificando formado do dataframe
df_order_reviews.shape

#verificando informações existentes no df
df_order_reviews.info()

#observando os dados
df_order_reviews.head()

#procurando por dados nulos
df_order_reviews.isnull()

df_order_reviews.isnull().sum()

"""Esta base possui muitos dados faltantes nas colunas referentes as mensagens. Possivelmente deve se relacionar aos comentarios das entregas. Uma vez que a maior parte dos pedidos não possuem comentários. Por enquanto, vamos tratar esses dados como 0."""

#tratando dos dados nulos
df_order_reviews.fillna(0,inplace=True)

df_order_reviews.isnull()

df_order_reviews.isnull().sum()

"""Certinho, funcionou!
Por hora é suficiente.
"""

# Removendo duplicatas
df_order_reviews = df_order_reviews.drop_duplicates()

"""###3.6 df_orders"""

#verificando formato do dataframe
df_orders.shape

#verificando como estao dispostas as informações
df_orders.info()

#observando os dados
df_orders.head(10)

#verificando a existencia de dados nulos
df_orders.isnull()

df_orders.isnull().sum()

"""Possui alguns bons dados faltantes. Vamos tentar visualizar onde eles estão.."""

df_orders.head(10)

df_orders.describe()

# Substituindo valores ausentes com 0 para colunas numéricas,
# e com o valor mais frequente para colunas não numéricas
for column in df_orders.columns:
    if pd.api.types.is_numeric_dtype(df_orders[column]):  # Verifica se a coluna é numérica
        # Preenche os valores ausentes com 0
        df_orders[column] = df_orders[column].fillna(0)
    else:
        # Preenche valores ausentes com o valor mais frequente (moda)
        df_orders[column] = df_orders[column].fillna(df_orders[column].mode()[0])

# Verificando novamente a quantidade de valores ausentes após a substituição
print(df_orders.isnull().sum())

# Removendo duplicatas
df_orders = df_orders.drop_duplicates()

"""###3.7 df_products"""

#verificando informações presentes no df
df_products.info()

#observando as informações
df_products.head(10)

#verificando existencia de dados nulos
df_products.isnull()

df_products.isnull().sum()

"""Possui dados nulos, iremos então trata-los."""

# Substituindo valores ausentes com 0 para colunas numéricas,
# e com o valor mais frequente para colunas não numéricas
for column in df_products.columns:
    if pd.api.types.is_numeric_dtype(df_products[column]):  # Verifica se a coluna é numérica
        # Preenche os valores ausentes com 0
        df_products[column] = df_products[column].fillna(0)
    else:
        # Preenche valores ausentes com o valor mais frequente (moda)
        df_products[column] = df_products[column].fillna(df_products[column].mode()[0])

# Verificando novamente a quantidade de valores ausentes após a substituição
print(df_products.isnull().sum())

df_products= df_products.drop_duplicates()

#utilzação de histogramas para visualização da distribuição dos valores para cada uma das colunas presentes no df
df_products.hist(bins=50, figsize=(20,15))
plt.show()

"""###3.8 df_sellers"""

#verificação do formato do df
df_sellers.shape

#informações presentes no df
df_sellers.info()

#visualização dos df
df_sellers.head(10)

#procura por dados nulos
df_sellers.isnull()

df_sellers.isnull().sum()

"""Não possui dados faltantes.

"""

df_sellers= df_sellers.drop_duplicates()

"""###3.9 df_products_category

"""

#verificação do formato do df
df_products_category.shape

#informações presentes no df
df_products_category.info()

#visualização do df
df_products_category.head(10)

#procura por dados nulos
df_products_category.isnull()

df_products_category.isnull().sum()

df_products_category= df_products_category.drop_duplicates()

"""##4. Criação de um único df

Para elaborar um unico df deve-se localizar nos df's existentes colunas em comum. Feito isso, utiliza-se o **Método Merge **. Em suma, o método merge do pandas é utilizado para combinar dois DataFrames com base em colunas em comum, funcionando de forma semelhante a um "JOIN" em SQL. Ele permite diferentes tipos de junção, como inner, left, right e outer, possibilitando unir dados de fontes distintas conforme chaves especificadas. É amplamente usado para integrar tabelas relacionadas em uma análise de dados, garantindo consistência e facilitando a construção de um modelo relacional.

####Método merge

Antes de aplicar ao df original, faz-se testes.
"""

df_order_items2 = df_order_items.copy()
teste = pd.merge(df_order_items2, df_order_payments, how = 'outer', on = 'order_id')

teste.head(15)

teste.info()

"""Certo, funcionou! Logo, vamos junta-se todas que possuem order id."""

df_com = pd.merge(df_order_items, df_order_payments, how = 'outer', on = 'order_id')

df_comple = pd.merge(df_com, df_order_reviews, how = 'outer', on = 'order_id')

df_completo =  pd.merge(df_comple, df_orders, how = 'outer', on = 'order_id')

df_completo.head(20)

df_completo.info()

df_orders.info()

"""Agora junta-se o novo df criado aos demais pelo costumer_id

"""

df2_com = pd.merge(df_completo, df_customers, how = 'outer', on = 'customer_id')

df2_com.head(7)

df2_com.info()

df2_com.shape

"""Feito isso, junta-se o dataframe em construção com o df de sellers, pelo seller_id

"""

df2_comple = pd.merge(df2_com, df_sellers , how = 'outer', on = 'seller_id')

df2_comple.info()

df2_comple.head(7)

df_products.info()

"""Agora, junta-se pelo produto_id."""

df2_completo = pd.merge(df2_comple, df_products , how = 'outer', on = 'product_id')

df2_completo.info()

df2_completo.head(7)

"""Feito isso, faz-se necessário a análise desse df, a fim de verificar se existem dados nulos e afins."""

#verificando dados faltantes
df2_completo.isnull()

df_completo.isnull().sum()

#observando novo df
x = df2_completo.info()

#visualizando todas colunas do dataframe
x = df2_completo.columns.values
for i in x:
    print(i)

#verificando a quantidade de valores unicos
df2_completo['product_id'].value_counts()

# Substituindo valores ausentes com 0 para colunas numéricas,
# e com o valor mais frequente para colunas não numéricas
for column in df2_completo.columns:
    if pd.api.types.is_numeric_dtype(df2_completo[column]):  # Verifica se a coluna é numérica
        # Preenche os valores ausentes com 0
        df2_completo[column] = df2_completo[column].fillna(0)
    else:
        # Preenche valores ausentes com o valor mais frequente (moda)
        df2_completo[column] = df2_completo[column].fillna(df2_completo[column].mode()[0])

# Verificando novamente a quantidade de valores ausentes após a substituição
print(df2_completo.isnull().sum())

df_final = df2_completo.copy()
df_final.info()

"""Por fim, normaliza-se os dados necessários."""

#Converter colunas de datas
colunas_datas = [
    'order_purchase_timestamp',
    'order_approved_at',
    'order_delivered_carrier_date',
    'order_delivered_customer_date',
    'order_estimated_delivery_date',
    'review_creation_date',
    'review_answer_timestamp'
]

for col in colunas_datas:
    if col in df_final.columns:
        df_final[col] = pd.to_datetime(df_final[col], errors='coerce')

#Converter colunas numéricas
colunas_numericas = [
    'payment_value',
    'freight_value',
    'price',
    'product_weight_g',
    'product_length_cm',
    'product_height_cm',
    'product_width_cm'
]

for col in colunas_numericas:
    if col in df_final.columns:
        df_final[col] = pd.to_numeric(df_final[col], errors='coerce')

#Verificar se ainda restaram valores nulos após as conversões
print("Valores nulos após tratamento:")
print(df_final[colunas_datas + colunas_numericas].isnull().sum())

df_final.info()

"""**OBS:. VALE RESSALTAR QUE O df_geolocation NÃO FOI INSERIDO POR NÃO TER CHAVE EM COMUM!**

Passos para **preparação dos dados**:

Na etapa de preparação dos dados, todos os arquivos CSV do dataset foram inicialmente carregados e analisados individualmente, a fim de compreender suas estruturas e relações. Em seguida, foi realizado o tratamento de valores nulos, com a remoção ou o preenchimento dos dados ausentes conforme a relevância de cada coluna para a análise. Também foram identificadas e eliminadas duplicatas, garantindo a integridade das informações. Após essas etapas, as tabelas foram integradas por meio de chaves comuns utilizando o método merge, formando um modelo relacional para análises mais aprofundadas. Com o DataFrame final consolidado, procedeu-se à padronização dos tipos de dados: as colunas de datas foram convertidas para o tipo datetime, viabilizando operações temporais como filtragem por período e cálculo de prazos de entrega; já as colunas numéricas, como valores de pagamento, frete e dimensões dos produtos, foram convertidas para os tipos float ou int, permitindo a realização de cálculos estatísticos e agregações com precisão. Esse tratamento final foi essencial para eliminar inconsistências e preparar os dados para a fase de análise exploratória.

##5. Visualização e compreensão dos dados
"""

# Histograma para verificar a distribuição dos dados
plt.figure(figsize=(20, 18))
df_final.hist(bins=50, figsize=(12, 12), grid=False)
plt.suptitle('Distribuição das Variáveis')
plt.tight_layout()
plt.show()

# Boxplot para identificar outliers em cada coluna
# Os pontos fora da caixa indicam outliers

plt.figure(figsize=(12, 6))
sns.boxplot(data=df_final)
plt.title('Boxplot para Identificação de Outliers')
plt.xticks(rotation=45)
plt.show()

# Selecionar apenas colunas numéricas para verificar correlação
df_numerico = df_final.select_dtypes(include=['float64', 'int64'])

# Matriz de Correlação
plt.figure(figsize=(10, 8))
sns.heatmap(df_numerico.corr(), annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)
plt.title('Matriz de Correlação entre Variáveis Numéricas')
plt.tight_layout()
plt.show()

fig, axes = plt.subplots(3, 4, figsize=(15, 10))
axes = axes.flatten()

# Loop para plotar cada variável em seu respectivo subgráfico
for i, col in enumerate(df_numerico.columns[:12]):  # Considerando as primeiras 12 colunas
    df_numerico[col].plot(kind='line', ax=axes[i], title=col)

# Ajustar o layout para que não sobreponha os títulos e labels
plt.tight_layout()
plt.show()

"""##6. Volume de pedidos por mês"""

#Volume de pedidos por mês

#Aqui, primeiro converte-se a coluna de data de compra para o período mensal (ano e mês), para agrupar os dados por mês.
#Feito isso agrupa-se o DataFrame pelo mês e conta o número de pedidos únicos (order_id) em cada mês.
df_final['order_month'] = df_final['order_purchase_timestamp'].dt.to_period('M')
pedidos_por_mes = df_final.groupby('order_month')['order_id'].nunique()

plt.figure(figsize=(12, 6))
pedidos_por_mes.plot(kind='bar', color='skyblue')
plt.title('Volume de Pedidos por Mês')
plt.xlabel('Mês')
plt.ylabel('Quantidade de Pedidos')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

"""Em relação a sazonalidade o gráfico de volume de pedidos por mês, entre setembro de 2016 e setembro de 2018, revela um padrão sazonal claro, com picos recorrentes em dezembro de cada ano, associados a períodos de festas como Natal e Réveillon, quando as vendas atingem seus maiores volumes. Além disso, observa-se uma queda consistente nos pedidos durante o primeiro trimestre (janeiro a março), possivelmente devido ao consumo reduzido após as compras de fim de ano. Essas variações regulares indicam sazonalidade, com períodos de alta e baixa demanda que se repetem anualmente, embora o volume geral de vendas apresente uma tendência de crescimento ao longo do período analisado.

##7. Distribuição do tempo de entrega dos pedidos
"""

#Distribuição do tempo de entrega dos pedidos
#Calcula-se o tempo de entrega em dias subtraindo a data da compra da data em que o pedido foi entregue ao cliente.
df_final['tempo_entrega'] = (df_final['order_delivered_customer_date'] - df_final['order_purchase_timestamp']).dt.days

plt.figure(figsize=(10, 5))
sns.histplot(df_final.loc[(df_final['tempo_entrega'] >= 0) & (df_final['tempo_entrega'] <= 60), 'tempo_entrega'], bins=30, kde=True, color='salmon')
plt.title('Distribuição do Tempo de Entrega (0 a 60 dias)')
plt.xlabel('Dias para Entrega')
plt.ylabel('Frequência')
plt.tight_layout()
plt.show()

"""##8.Valor do frete X distância de entrega?

O df_final nao tem lat long, mas tem no df_geolocation que nao foi adicionado ao df_final. Utiliza-se portanto as informações que estão no df_geolocation, e assim calcula-se a distancia.
"""

#faz um copia do geolocation
geo = df_geolocation.copy()

# Calcular média de lat e long para cada prefixo de CEP
geo_grouped = geo.groupby('geolocation_zip_code_prefix').agg({
    'geolocation_lat': 'mean',
    'geolocation_lng': 'mean'
}).reset_index()

# Renomear colunas para evitar confusão
geo_grouped.columns = ['zip_code_prefix', 'lat', 'lng']

# Juntar latitude e longitude médias do cliente
df_final = df_final.merge(
    geo_grouped,
    left_on='customer_zip_code_prefix',
    right_on='zip_code_prefix',
    how='left'
).rename(columns={'lat': 'customer_lat', 'lng': 'customer_lng'}).drop('zip_code_prefix', axis=1)

# Juntar latitude e longitude médias do vendedor
df_final = df_final.merge(
    geo_grouped,
    left_on='seller_zip_code_prefix',
    right_on='zip_code_prefix',
    how='left'
).rename(columns={'lat': 'seller_lat', 'lng': 'seller_lng'}).drop('zip_code_prefix', axis=1)

# Função para calcular distância
def calculo_distancia(lat1, lon1, lat2, lon2):
    R = 6371  # raio da Terra em km
    phi1 = np.radians(lat1)
    phi2 = np.radians(lat2)
    delta_phi = np.radians(lat2 - lat1)
    delta_lambda = np.radians(lon2 - lon1)
    a = np.sin(delta_phi/2)**2 + np.cos(phi1) * np.cos(phi2) * np.sin(delta_lambda/2)**2
    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1 - a))
    return R * c

# Calcular a distância em km para cada linha
df_final['distance_km'] = calculo_distancia(
    df_final['customer_lat'], df_final['customer_lng'],
    df_final['seller_lat'], df_final['seller_lng']
)

#Removervalores extremos para melhorar a visualização
df_plot = df_final[(df_final['distance_km'] < 2000) & (df_final['freight_value'] < 500)]

plt.figure(figsize=(10, 6))
sns.scatterplot(data=df_plot, x='distance_km', y='freight_value', alpha=0.5)
plt.title('Relação entre Distância e Valor do Frete')
plt.xlabel('Distância (km)')
plt.ylabel('Valor do Frete (R$)')
plt.grid(True)
plt.tight_layout()
plt.show()

"""##9. Categorias de produtos mais vendidas em termos de faturamento"""

#Categorias de produtos mais vendidas em termos de faturamento
#Aqui agrupa-se o DataFrame pela categoria do produto somando o preço total de cada categoria.
#Feito isso, ordena-se de forma decrescente para obter as categorias que mais faturaram. Por fim, faz-se o plot.

faturamento_categoria = df_final.groupby('product_category_name')['price'].sum().sort_values(ascending=False).head(10)

plt.figure(figsize=(12, 6))
faturamento_categoria.plot(kind='bar', color='mediumseagreen')
plt.title('Top 10 Categorias com Maior Faturamento')
plt.xlabel('Categoria do Produto')
plt.ylabel('Faturamento Total (R$)')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

"""##10.  Estados brasileiros que possuem o maior valor médio de pedido"""

# estados brasileiros que possuem o maior valor médio de pedido
#Aqui, agrupa-se DataFrame pelo estado do cliente e calcula-se a média dos preços dos pedidos em cada estado.
#Assim, Ordena-se para obter os 10 estados com maior valor médio.
valor_medio_estado = df_final.groupby('customer_state')['price'].mean().sort_values(ascending=False).head(10)

plt.figure(figsize=(12, 6))
valor_medio_estado.plot(kind='bar', color='red')
plt.title('Top 10 Estados com Maior Valor Médio de Pedido')
plt.xlabel('Estado')
plt.ylabel('Valor Médio do Pedido (R$)')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

"""##11.  Análise de Retenção.

**Análise de Retenção:** Calcule a taxa de clientes recorrentes. Considere um cliente recorrente aquele que fez mais de um pedido no período analisado. Quais insights podemos extrair destes dados?
"""

#Conta-se quantos pedidos cada cliente fez usando o campo customer_unique_id.
#Considera-se cliente recorrente aquele que fez mais de um pedido.

clientes_pedidos = df_final.groupby('customer_unique_id')['order_id'].nunique()
clientes_recorrentes = clientes_pedidos[clientes_pedidos > 1].count()
clientes_unicos = clientes_pedidos.count()
taxa_recorrencia = clientes_recorrentes / clientes_unicos
print('Taxa de recorrencia igual a: ', taxa_recorrencia)

"""Insigths: Apenas 3,1% dos clientes realizaram mais de um pedido no período analisado.

A análise de retenção revelou que apenas 3,1% dos clientes realizaram mais de um pedido durante o período avaliado, o que indica uma taxa de recorrência bastante baixa. Esse resultado sugere que a maior parte dos consumidores realiza compras únicas, o que pode estar relacionado a diversos fatores, como o tipo de produto oferecido (itens de compra não frequente), a ausência de estratégias de fidelização, ou possíveis problemas de experiência do cliente, como atrasos na entrega ou atendimento insatisfatório. Esse cenário apresenta uma oportunidade significativa de melhoria: investir em ações de fidelização, como programas de recompensas, cupons de desconto para uma segunda compra e campanhas de e-mail marketing personalizadas, pode aumentar o engajamento e estimular novos pedidos. Além disso, é fundamental compreender melhor quem são os clientes recorrentes – analisando seus perfis, comportamentos e preferências – para que estratégias direcionadas possam ser aplicadas a consumidores com características semelhantes.

##12. Predição de atraso
"""

df_modelo = df_final.copy()

# Criando a variável 'atrasado' (1 se atrasou, 0 se não atrasou)
df_modelo['atrasado'] = (df_modelo['order_delivered_customer_date'] > df_modelo['order_estimated_delivery_date']).astype(int)

# Definindo as features relevantes
features = [
    'price', 'freight_value', 'payment_value', 'payment_installments', 'review_score',
    'product_photos_qty', 'product_weight_g', 'product_length_cm',
    'product_height_cm', 'product_width_cm'
]

# Removendo registros com valores nulos
df_modelo = df_modelo.dropna(subset=features + ['atrasado'])

# Separando X e y
X = df_modelo[features]
y = df_modelo['atrasado']

# Normalizando os dados
scaler = StandardScaler()
X_normalizado = scaler.fit_transform(X)

# Separando treino e teste
X_train, X_test, y_train, y_test = train_test_split(X_normalizado, y, test_size=0.2, random_state=42)

# Treinando modelo Random Forest
modelo_rf = RandomForestClassifier(random_state=42)
modelo_rf.fit(X_train, y_train)

# Fazendo previsões
y_pred = modelo_rf.predict(X_test)

# Avaliação
accuracy = accuracy_score(y_test, y_pred)
relatorio = classification_report(y_test, y_pred, output_dict=True)
matriz_confusao = confusion_matrix(y_test, y_pred)

# Matriz de Confusão
plt.figure(figsize=(6, 4))
sns.heatmap(matriz_confusao, annot=True, fmt="d", cmap="Blues", xticklabels=["No Delay", "Delayed"], yticklabels=["No Delay", "Delayed"])
plt.xlabel("Predito")
plt.ylabel("Real")
plt.title("Matriz de Confusão - Previsão de Atraso na Entrega")
plt.show()

# Relatório resumido
print(f"Acurácia: {accuracy:.2%}")
print("\nRelatório de Classificação:")
print(classification_report(y_test, y_pred, target_names=["No Delay", "Delayed"]))

"""**Avaliação de performance:** O modelo atingiu uma acurácia de **93,26%,** indicando um bom desempenho geral na classificação das entregas como atrasadas ou não. No entanto, a análise mais detalhada revela um desbalanceamento de desempenho entre as classes: enquanto a classe "No Delay" apresenta alta precisão (0,94) e recall (0,99), a classe "Delayed" tem desempenho significativamente inferior, com precisão de apenas 0,65 e recall de 0,27. Isso significa que o modelo tem dificuldade em identificar corretamente as entregas atrasadas, classificando muitas delas incorretamente como pontuais. Apesar da alta acurácia geral, o baixo recall para atrasos sugere que o modelo não é confiável para aplicações que exigem alta sensibilidade na detecção de atrasos, sendo recomendável aplicar técnicas de balanceamento de classes ou ajustar os critérios de decisão para melhorar esse aspecto.

##13. Segmentação de Clientes
"""

df_clientes = df_final.groupby('customer_unique_id').agg({
    'payment_value': 'sum',
    'freight_value': 'sum',
    'price': 'sum',
    'review_score': 'mean',
    'payment_installments': 'mean',
    'order_id': 'count'
}).rename(columns={'order_id': 'order_count'})

# Normalização
scaler = StandardScaler()
X_scaled = scaler.fit_transform(df_clientes)

# Clustering com KMeans
kmeans = KMeans(n_clusters=3, random_state=42)
df_clientes['cluster'] = kmeans.fit_predict(X_scaled)

df_cluster_analise = df_clientes.groupby('cluster').mean()
print(df_cluster_analise)

"""**Análise:**A segmentação identificou três perfis distintos de clientes: o Cluster 0 é composto por clientes econômicos que fazem poucas compras, pagam pouco e apresentam alta satisfação, sugerindo estratégias de fidelização leve e ofertas de combos econômicos; o Cluster 1 reúne clientes premium que gastam bastante, parcelam mais e realizam mais pedidos, porém com avaliações mais baixas, indicando a necessidade de melhorar a experiência de entrega e oferecer benefícios como frete grátis e atendimento personalizado; já o Cluster 2 agrupa clientes insatisfeitos com baixo gasto e poucas compras, demandando ações focadas em entender e corrigir suas reclamações, além de campanhas de reengajamento com descontos para recuperar a confiança e estimular novas compras.

##14. Análise de Satisfação
"""

#Calculo da media por categoria
media_avaliacao_categoria = df_final.groupby('product_category_name')['review_score'].mean().sort_values()

# Média da avaliação vs tempo de entrega (em dias)
df_final['delivery_time_days'] = (df_final['order_delivered_customer_date'] - df_final['order_purchase_timestamp']).dt.days
media_avaliacao_tempo = df_final.groupby('delivery_time_days')['review_score'].mean()

# Correlação entre avaliação, valor do pedido e tempo de entrega
correlacoes = df_final[['review_score', 'payment_value', 'delivery_time_days']].corr()

plt.figure(figsize=(12,6))
sns.barplot(x=media_avaliacao_categoria.index, y=media_avaliacao_categoria.values, palette='viridis')
plt.xticks(rotation=90)
plt.title('Avaliação Média por Categoria do Produto')
plt.ylabel('Nota Média de Avaliação')
plt.xlabel('Categoria do Produto')
plt.tight_layout()
plt.show()

"""**Analise**: A análise revela que categorias específicas de produtos apresentam diferenças na média de avaliações, indicando que alguns segmentos podem ter mais problemas ou expectativas diferentes.

##15. Visualização e dashboards

###Um dashboard geral que mostre a evolução das vendas ao longo do tempo, com filtros por estado e categoria de produto
"""

from ipywidgets import interact

df_final['order_purchase_timestamp'] = pd.to_datetime(df_final['order_purchase_timestamp'])
df_final['order_month'] = df_final['order_purchase_timestamp'].dt.to_period('M').dt.to_timestamp()

def plot_vendas(estado, categoria):
    df_filtrado = df_final[(df_final['customer_state'] == estado) & (df_final['product_category_name'] == categoria)]
    vendas_mensais = df_filtrado.groupby('order_month')['payment_value'].sum().reset_index()
    plt.figure(figsize=(12,6))
    sns.lineplot(data=vendas_mensais, x='order_month', y='payment_value')
    plt.title(f'Vendas em {estado} - Categoria: {categoria}')
    plt.xlabel('Mês')
    plt.ylabel('Valor Total de Vendas')
    plt.xticks(rotation=45)
    plt.grid(True)
    plt.show()

estados = sorted(df_final['customer_state'].unique())
categorias = sorted(df_final['product_category_name'].unique())

interact(plot_vendas, estado=estados, categoria=categorias)

"""###Um mapa de calor mostrando a concentração de vendas por região/estado do Brasil


"""

import requests
# Exemplo: seu df com vendas por estado
vendas_estado = df_final.groupby('customer_state')['payment_value'].sum().reset_index()

# Baixar geojson dos estados do Brasil
url_geojson = 'https://raw.githubusercontent.com/codeforamerica/click_that_hood/master/public/data/brazil-states.geojson'
br_states = requests.get(url_geojson).json()

# Conferir o formato das propriedades dos estados no geojson
# Normalmente, a propriedade é "sigla" ou "name"
for feature in br_states['features']:
    print(feature['properties'])

# Mapear sua coluna 'customer_state' para 'sigla' do geojson

# Criar o mapa
fig = px.choropleth(
    vendas_estado,
    geojson=br_states,
    locations='customer_state',
    featureidkey='properties.sigla',
    color='payment_value',
    color_continuous_scale='Viridis',
    labels={'payment_value': 'Total Vendas'},
    title='Concentração de Vendas por Estado - Brasil'
)

fig.update_geos(fitbounds="locations", visible=False)
fig.show()

"""###Um conjunto de gráficos que apresente a relação entre avaliação do cliente e tempo de entrega


"""

# Filtrar para delivery_time_days >= 0 e <= 60 para foco na faixa relevante
df_filtered = df_final[(df_final['delivery_time_days'] >= 0) & (df_final['delivery_time_days'] <= 60)]

fig, axs = plt.subplots(1, 2, figsize=(16,6))

# Boxplot com menos bins e labels legíveis
bins = pd.cut(df_filtered['delivery_time_days'], bins=5)
sns.boxplot(x=bins, y='review_score', data=df_filtered, ax=axs[0])
axs[0].set_title('Distribuição da Avaliação por Tempo de Entrega (até 60 dias)')
axs[0].set_xlabel('Dias para entrega (intervalos)')
axs[0].set_ylabel('Avaliação')
axs[0].tick_params(axis='x', rotation=45)

# Scatter plot com jitter para evitar sobreposição
sample = df_filtered.sample(min(1000, len(df_filtered)), random_state=42).copy()
jitter = np.random.uniform(-0.5, 0.5, size=sample.shape[0])
sample['delivery_time_days_jitter'] = sample['delivery_time_days'] + jitter

sns.scatterplot(x='delivery_time_days_jitter', y='review_score', data=sample, ax=axs[1], alpha=0.6, edgecolor=None)
axs[1].set_title('Avaliação vs Tempo de Entrega (Amostra com jitter)')
axs[1].set_xlabel('Dias para entrega (com jitter)')
axs[1].set_ylabel('Avaliação')
axs[1].set_xlim(-1, 61)  # Para focar no intervalo de interesse

plt.tight_layout()
plt.show()

"""###Um dashboard de análise dos vendedores, mostrando quais têm melhor desempenho em termos de volume de vendas, satisfação do cliente e tempo de entrega"""

# Converter para datetime (caso ainda não tenha feito)
df_final['order_delivered_customer_date'] = pd.to_datetime(df_final['order_delivered_customer_date'])
df_final['order_purchase_timestamp'] = pd.to_datetime(df_final['order_purchase_timestamp'])

# Calcular tempo de entrega em dias
df_final['delivery_time_days'] = (df_final['order_delivered_customer_date'] - df_final['order_purchase_timestamp']).dt.days

# Remover linhas com valores nulos e com delivery_time_days negativo ou zero (tempo inválido)
df_vendedores = df_final.dropna(subset=['payment_value', 'review_score', 'delivery_time_days', 'seller_id'])
df_vendedores = df_vendedores[df_vendedores['delivery_time_days'] > 0]

# Agrupar por vendedor
vendedores_agg = df_vendedores.groupby('seller_id').agg({
    'payment_value': 'sum',
    'review_score': 'mean',
    'delivery_time_days': 'mean'
}).reset_index()

# Plotar gráfico
fig = px.scatter(
    vendedores_agg,
    x='payment_value',
    y='review_score',
    size='delivery_time_days',
    hover_name='seller_id',
    title='Desempenho dos Vendedores',
    labels={
        'payment_value': 'Volume de Vendas',
        'review_score': 'Satisfação Média',
        'delivery_time_days': 'Tempo Médio de Entrega (dias)'
    },
    size_max=20
)

fig.show()

#solucao para colocar no github
!jupyter nbextension enable --py widgetsnbextension --sys-prefix